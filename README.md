# PecularitiesWithNegativeCertainty
This will be a public documentation seperate from my main research, showing weird situations I get when experimenting with negative certainty and other oddities in machine learning.

## Quantified Uncertainty
~~~
Level 1 Uncertainty
I'm confident it is not [ gerbils Gerbils dont use hamster wheels. ] as it has only -0.054450000000000005 probability.
The probability is either to low or to large, so I can't determine exactly.
I'm confident it is not [ gerbils Gerbils dont use hamster wheels. ] because it has 0.041099999999999984.
I'm confident it is not [ gerbils Gerbils dont use hamster wheels. ] because it has 0.1911.
I'm confident it is not [ dogs Dogs are mans best friend. ] as it has only -0.054450000000000005 probability.
The probability is either to low or to large, so I can't determine exactly.
I'm confident it is not [ dogs Dogs are mans best friend. ] because it has 0.041099999999999984.
I'm confident it is not [ dogs Dogs are mans best friend. ] because it has 0.1911.
I'm less unconfident it is not [ dogs Dogs are mans best friend. ] because it has 0.34109999999999996.

Level 2 Uncertainty
I'm confident it is not [ dogs Dogs are mans best friend. ] as it has only -0.035937000000000004 probability.
The probability is either to low or to large, so I can't determine exactly.
I'm confident it is not [ dogs Dogs are mans best friend. ] because it has 0.07812599999999999.
I'm confident it is not [ dogs Dogs are mans best friend. ] because it has 0.228126.
I'm confident it is not [ dogs Dogs are mans best friend. ] as it has only -0.035937000000000004 probability.
The probability is either to low or to large, so I can't determine exactly.
I'm confident it is not [ dogs Dogs are mans best friend. ] because it has 0.07812599999999999.
~~~
